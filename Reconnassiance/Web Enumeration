#----GoBuster (https://github.com/OJ/gobuster)
- Can be utilized to enumerate subdomains as well as subdirectories on a web server, for example:

gobuster dir -u <webpage> -w <wordlist.txt> 

#----Typical Browser Tools (i.e. View Source, Inspector, Debugger etc.)
1) A good place to start is simply the URL and looking for typical subdirectories such as /customers, /customers/login, /contact, /news etc. These subdirectories can give you some idea regarding what functionalities are on the website. 

2) Page Source - This is the human-readable code returned to our browser/client, given by the web server each time we make a request. The returned code is:
- HTML (What to display)
- CSS (How to format it)
- Javascript (Adds some interactivity to the page so it's not static) 

You may be looking for shit like:
- Comments left by devs
- Sus. Javascript code and/or .js files
- Sus links, designated by "<a" (Anchor tag), followed by "href". i.e. <a href="/news">News</a>
- Access to directories, like /assets etc. 
- Framework information about the version/type of framework used to build the site e.g. django etc. Can check the version and see if there's known vuls

3) Developer Tools - Inspector
- The issue with View Source is its more of a Snapshot of a webpage at a given time. Instead, what if we want to dynamically view the contents of a page at any given time, and dynamically have it update as the page changes. Inspector is bascially a live-view of what is currently on the website that we're looking at, rather than a snapshot of whatever we are seeing when we request the website (page source). With inspector, we can also EDIT and INTERACT with elements on the page.

- With inspector/inspect element, we can edit the HTML we're looking at. For example, if something is blocked behind a paywall, you may be able to bypass it by changing some "blocked" value in the html to "none" or smth of the sort. 

- This only changes shit locally on our browser. Not on the svr-side obviously. 

4) Debugger 
- This tool is used to debug the javascript on a page by web devs, but can be used in pentesting as well to dig into the functioning of javascript code. In Edge this is the "Sources" option in the dev tools. On the left, we can see a heirarchal layout of stuff being used by the website. We also have the option to input breakpoints (similar to pycharm etc.) while the webpage loads/does stuff, so we can examine its functioning step by step. To create a breakpoint, simply click on the line number when looking at the code. 

- Oftentimes, javascript files are in one line. Use the pretty print option at the bottom of the sources page (Looks like {}) to format it properly.

5) Network
- The network tab can be used to keep track of every external request a webpage makes. 
- For example, if there's a form you need to fill out, you can fill it out with random data, hit submit, and see where that data goes. 


#----Content Discovery
WRT Content Discovery, we're talking about non-obvious things on a website that aren't often intended for public use. For example, access to configuration files, Admin portals etc. You can find content via 3 different methods:

1) Manually
- Robots.txt: This is a text file that tells search engines wich pages they are and aren't allowed to show on their search engine results. Can help against web crawlers and web archiving tools from looking at/archiving particular subdirectories w/i a website. However, this also just straight up gives us a list of subdirectories that are indeed present. 

- Favicons: When you open a webpage, on the tab that has the webpage open, at the left of the tab, is a little icon. E.g. for youtube it's the youtube icon etc. Some devs forget to change this. By default, the default favicon represents the framework used to create the site. You can grab the icon from 'page source,' run an MD5 hash on the icon file, and see if its in here to get the framework (https://wiki.owasp.org/index.php/OWASP_favicon_database)

- Sitemap.xml: The sitemap.xml file is sort of the opposite of robots.txt. It lists every file the owner wants to be listed by a search engine. Found via https://site.com/sitemap.xml 

- HTTP Headers: HTTP Responses from Web servers oft have header data that contain some useful info, such as webserver type, version etc. 

2) OSINT
- Google Dorking: Advanced Google search engine queries to basically just craft really specific queries, such as all pdf files but only on websites that start with tryhackme.com or smth. Some common Google Dorks: (https://hackr.io/blog/google-dorks-cheat-sheet)
site: -> Return only results from specific website. e.g. site:tryhackme.com
inurl: -> Return results that have the specified word in the URL e.g. inurl:admin
filetype: -> Return specific file extensions e.g. filetype:pdf
intitle: -> Return results that contain the speciifed word in the title e.g. intitle:admin

- Wappalyzer: Online tool/browser Extension to help determine info such as frameworks, and more. (https://www.wappalyzer.com/)

- Wayback Machine: You're very familiar

- GitHub: Common place to find source code for websites, and other website info like company names, or older versions of websites. As its based on Git (A version control system) you can find stuff like older source code and other info

- s3 Buckets: http(s)://{name}.s3.amazonaws.com Sometimes S3 storage buckets are public/incorrect permissions, and you may be able to access them. 


3) Automations
This refers to tools like Gobuster etc. w/ a wordlist to find subdomains.

ffuf -w /usr/share/wordlists/SecLists/Discovery/Web-Content/common.txt -u http://10.10.108.72/FUZZ
dirb http://10.10.108.72/ /usr/share/wordlists/SecLists/Discovery/Web-Content/common.txt
gobuster dir --url http://10.10.108.72/ -w /usr/share/wordlists/SecLists/Discovery/Web-Content/common.txt
